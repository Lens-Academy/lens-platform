---
phase: 06-chat-evaluation
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - core/promptlab/regenerate.py
  - web_api/routes/promptlab.py
  - main.py
autonomous: true

must_haves:
  truths:
    - "GET /api/promptlab/fixtures returns list of fixture metadata"
    - "GET /api/promptlab/fixtures/{name} returns full fixture data"
    - "POST /api/promptlab/regenerate streams SSE events with text and thinking blocks"
    - "POST /api/promptlab/continue streams SSE events for follow-up conversation"
    - "All Prompt Lab endpoints require facilitator/admin authentication"
    - "Prompt Lab never writes to chat_sessions, assessment_responses, or assessment_scores tables"
  artifacts:
    - path: "core/promptlab/regenerate.py"
      provides: "LLM regeneration with thinking support"
      exports: ["regenerate_response", "continue_conversation"]
    - path: "web_api/routes/promptlab.py"
      provides: "Prompt Lab API endpoints"
      exports: ["router"]
  key_links:
    - from: "core/promptlab/regenerate.py"
      to: "core/modules/llm.py"
      via: "import stream_chat"
      pattern: "from core\\.modules\\.llm import stream_chat"
    - from: "web_api/routes/promptlab.py"
      to: "core/promptlab"
      via: "import fixtures and regenerate"
      pattern: "from core\\.promptlab"
    - from: "web_api/routes/promptlab.py"
      to: "web_api/routes/facilitator.py"
      via: "reuse auth pattern"
      pattern: "get_db_user_or_403|get_current_user"
    - from: "main.py"
      to: "web_api/routes/promptlab.py"
      via: "router registration"
      pattern: "include_router.*promptlab"
---

<objective>
Create the Prompt Lab backend: regeneration module with thinking support and all API endpoints.

Purpose: Provide the backend infrastructure for regenerating AI responses with custom system prompts, streaming via SSE with thinking blocks, and continuing conversations -- all without writing to production database tables.

Output: core/promptlab/regenerate.py, web_api/routes/promptlab.py with 4 endpoints, router registered in main.py.
</objective>

<execution_context>
@/home/penguin/.claude/get-shit-done/workflows/execute-plan.md
@/home/penguin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-chat-evaluation/06-CONTEXT.md
@.planning/phases/06-chat-evaluation/06-RESEARCH.md
@.planning/phases/06-chat-evaluation/06-01-SUMMARY.md

@core/modules/llm.py
@core/modules/chat.py
@web_api/routes/module.py
@web_api/routes/facilitator.py
@main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create core/promptlab/regenerate.py</name>
  <files>
    core/promptlab/regenerate.py
    core/promptlab/__init__.py
  </files>
  <action>
    Create `core/promptlab/regenerate.py` -- the LLM interaction module for Prompt Lab. This wraps `stream_chat()` from `core/modules/llm.py` with Prompt Lab-specific concerns: custom system prompts, thinking/chain-of-thought support, and no database writes.

    Per INFRA-03: imports from core/modules/llm.py directly, does NOT import from chat.py or scoring.py.
    Per INFRA-04: does NOT import database modules or write to any tables.

    Implementation:

    ```python
    """Prompt Lab LLM regeneration with thinking support."""

    from typing import AsyncIterator
    from core.modules.llm import stream_chat, DEFAULT_PROVIDER
    from litellm import acompletion


    async def regenerate_response(
        messages: list[dict],
        system_prompt: str,
        enable_thinking: bool = False,
        thinking_budget: int = 4096,
        provider: str | None = None,
        max_tokens: int = 2048,
    ) -> AsyncIterator[dict]:
        """
        Regenerate an AI response with a custom system prompt.

        This is the core Prompt Lab function. Unlike the production chat flow,
        it accepts an arbitrary system prompt (editable by facilitator) and
        optionally enables chain-of-thought/thinking blocks.

        Args:
            messages: Conversation history up to (but not including) the response to generate.
                      List of {"role": "user"|"assistant", "content": str}.
            system_prompt: The full system prompt (base + instructions), editable by facilitator.
            enable_thinking: Whether to request thinking/chain-of-thought from the LLM.
            thinking_budget: Token budget for thinking blocks (only used if enable_thinking=True).
            provider: LLM provider string. If None, uses DEFAULT_PROVIDER.
            max_tokens: Maximum tokens in response.

        Yields:
            Normalized events:
            - {"type": "thinking", "content": str} for thinking/CoT chunks
            - {"type": "text", "content": str} for text chunks
            - {"type": "done"} when complete
            - {"type": "error", "message": str} on error

        Note: Does NOT write to any database. The caller (API route) is responsible
        for state management, which in Prompt Lab is all client-side.
        """
    ```

    For the implementation:
    - If `enable_thinking` is False, delegate directly to `stream_chat()` and yield its events as-is. This is the simple case.
    - If `enable_thinking` is True, call `acompletion` from litellm directly (not through `stream_chat()`) with the `thinking` parameter: `thinking={"type": "enabled", "budget_tokens": thinking_budget}`. Build messages the same way stream_chat does: prepend system message. Stream the response and:
      - For chunks with `delta.content`, yield `{"type": "text", "content": delta.content}`
      - For chunks with thinking content (check `delta.thinking` or `delta.reasoning_content` -- LiteLLM normalizes this differently per provider; use `getattr(delta, "reasoning_content", None)` or check the `delta` dict), yield `{"type": "thinking", "content": ...}`
      - At the end yield `{"type": "done"}`
    - Wrap the entire stream in try/except. On error, yield `{"type": "error", "message": str(e)}` then `{"type": "done"}`.

    IMPORTANT: The thinking parameter support in LiteLLM may vary. The implementation should try the `thinking` parameter approach first (as documented in LiteLLM docs for Anthropic extended thinking). If that doesn't work at runtime, the error will be caught and returned gracefully. Add a comment noting this may need adjustment based on the actual LiteLLM version behavior.

    Also create `continue_conversation()` with the same signature -- it's functionally identical to `regenerate_response()` (takes messages + system_prompt, streams response). The difference is semantic: regenerate replaces an existing message, continue adds after the last message. Both call the same underlying LLM. Just make `continue_conversation` call `regenerate_response` internally.

    Update `core/promptlab/__init__.py` to also export `regenerate_response` and `continue_conversation`.
  </action>
  <verify>
    Run `cd /home/penguin/code/lens-platform/ws2 && .venv/bin/python -c "from core.promptlab import regenerate_response, continue_conversation, list_fixtures, load_fixture; print('all imports ok')"` -- must succeed.
    Run `cd /home/penguin/code/lens-platform/ws2 && ruff check core/promptlab/ && ruff format --check core/promptlab/` -- must pass.
  </verify>
  <done>
    core/promptlab/regenerate.py exists with regenerate_response() and continue_conversation() async generators. Both are importable from core.promptlab. No imports from chat.py, scoring.py, or database modules.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create API routes and register router</name>
  <files>
    web_api/routes/promptlab.py
    main.py
  </files>
  <action>
    Create `web_api/routes/promptlab.py` with 4 endpoints and register the router in main.py.

    **Auth pattern:** Reuse the facilitator auth pattern from `web_api/routes/facilitator.py`. Import `get_current_user` from `web_api.auth` and create a local `get_facilitator_user` dependency (or import and reuse `get_db_user_or_403` from facilitator.py). All endpoints must check that the user is a facilitator or admin -- non-facilitators get 403.

    **Endpoints:**

    1. `GET /api/promptlab/fixtures` -- List available fixtures
       - Auth: facilitator required
       - Calls `list_fixtures()` from core.promptlab
       - Returns `{"fixtures": [{"name": str, "module": str, "description": str}, ...]}`

    2. `GET /api/promptlab/fixtures/{name}` -- Load a specific fixture
       - Auth: facilitator required
       - `name` is URL-encoded fixture name (use `urllib.parse.unquote` to decode)
       - Calls `load_fixture(name)` from core.promptlab
       - Returns full fixture JSON, or 404 if not found

    3. `POST /api/promptlab/regenerate` -- Regenerate AI response (SSE stream)
       - Auth: facilitator required
       - Request body (Pydantic model `RegenerateRequest`):
         ```python
         class RegenerateRequest(BaseModel):
             messages: list[dict]       # Conversation up to the point to regenerate
             systemPrompt: str          # Full edited system prompt
             enableThinking: bool = False  # Whether to include CoT
         ```
       - Calls `regenerate_response()` from core.promptlab
       - Returns `StreamingResponse` with `media_type="text/event-stream"` and headers `Cache-Control: no-cache`, `Connection: keep-alive`, `X-Accel-Buffering: no` (the X-Accel-Buffering header prevents nginx/reverse proxy buffering)
       - SSE format: `data: {"type": "text|thinking|done|error", ...}\n\n`
       - Follow exact same SSE pattern as `web_api/routes/module.py` `event_generator()`

    4. `POST /api/promptlab/continue` -- Continue conversation with follow-up (SSE stream)
       - Auth: facilitator required
       - Request body (Pydantic model `ContinueRequest`):
         ```python
         class ContinueRequest(BaseModel):
             messages: list[dict]       # Full conversation including the follow-up user message
             systemPrompt: str          # Current system prompt
             enableThinking: bool = False
         ```
       - Same streaming pattern as regenerate
       - Calls `continue_conversation()` from core.promptlab

    **Router setup:**
    ```python
    router = APIRouter(prefix="/api/promptlab", tags=["promptlab"])
    ```

    **Register in main.py:**
    - Add import: `from web_api.routes.promptlab import router as promptlab_router`
    - Add: `app.include_router(promptlab_router)` alongside the other router inclusions
    - Place the import and include near the facilitator router lines for logical grouping

    **Critical: No database writes.** The regenerate and continue endpoints do NOT save messages to any table. All conversation state is managed client-side in the Prompt Lab frontend.
  </action>
  <verify>
    Run `cd /home/penguin/code/lens-platform/ws2 && ruff check web_api/routes/promptlab.py && ruff format --check web_api/routes/promptlab.py` -- must pass.
    Run `cd /home/penguin/code/lens-platform/ws2 && .venv/bin/python -c "from web_api.routes.promptlab import router; print(f'Router prefix: {router.prefix}, routes: {len(router.routes)}')"` -- must show prefix /api/promptlab and 4 routes.
    Run `cd /home/penguin/code/lens-platform/ws2 && ruff check main.py` -- must pass (router registration didn't break main.py).
  </verify>
  <done>
    web_api/routes/promptlab.py exists with 4 endpoints (GET fixtures list, GET fixture by name, POST regenerate, POST continue), all requiring facilitator auth. Router registered in main.py. No database writes in any endpoint. Python linting passes.
  </done>
</task>

</tasks>

<verification>
1. `ruff check core/promptlab/ web_api/routes/promptlab.py` passes
2. All 4 API routes exist with correct HTTP methods and paths
3. Auth check is present on every endpoint (facilitator/admin required)
4. No imports from chat.py, scoring.py, or database write operations in promptlab code
5. Router is registered in main.py
6. SSE streaming endpoints use correct headers (text/event-stream, no-cache, keep-alive)
</verification>

<success_criteria>
- GET /api/promptlab/fixtures endpoint exists and returns fixture list
- GET /api/promptlab/fixtures/{name} endpoint exists and returns fixture data or 404
- POST /api/promptlab/regenerate streams SSE with text/thinking/done events
- POST /api/promptlab/continue streams SSE with text/thinking/done events
- All endpoints require facilitator authentication
- No database writes occur in any Prompt Lab code path
- Router registered in main.py
- All Python linting passes
</success_criteria>

<output>
After completion, create `.planning/phases/06-chat-evaluation/06-02-SUMMARY.md`
</output>
