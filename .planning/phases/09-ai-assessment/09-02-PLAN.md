---
phase: 09-ai-assessment
plan: 02
type: tdd
wave: 2
depends_on: ["09-01"]
files_modified:
  - web_api/routes/assessments.py
  - web_api/tests/test_assessments_scoring.py
autonomous: true

must_haves:
  truths:
    - "When a response is marked complete (completed_at set), AI scoring is triggered automatically in the background"
    - "Scoring is NOT triggered for draft saves (PATCH without completed_at)"
    - "Scoring is NOT triggered for initial POST creates (which are always drafts)"
    - "The PATCH response returns immediately without waiting for scoring to complete"
    - "AI scores are never exposed in any API response to the client"
  artifacts:
    - path: "web_api/routes/assessments.py"
      provides: "Scoring trigger wired into PATCH endpoint"
      contains: "enqueue_scoring"
    - path: "web_api/tests/test_assessments_scoring.py"
      provides: "Tests for scoring trigger conditions"
      contains: "class TestScoringTrigger"
  key_links:
    - from: "web_api/routes/assessments.py"
      to: "core/scoring.py"
      via: "enqueue_scoring call after completed_at is set"
      pattern: "from core\\.scoring import enqueue_scoring"
---

<objective>
Wire the AI scoring trigger into the assessment submission flow using TDD: RED tests for trigger conditions first, then GREEN implementation.

Purpose: The scoring module from Plan 01 exists but is not connected to anything. This plan connects it to the PATCH endpoint so that when a student finishes an answer (completed_at transitions from null to a timestamp), scoring fires automatically. The trigger must be precise: only on completion, never on draft saves, and never blocking the API response. The trigger logic is the most important correctness property of the whole phase and must be test-driven.

Output: `web_api/tests/test_assessments_scoring.py` (new, written FIRST), `web_api/routes/assessments.py` (modified)
</objective>

<execution_context>
@/home/penguin/.claude/get-shit-done/workflows/execute-plan.md
@/home/penguin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-ai-assessment/09-RESEARCH.md
@.planning/phases/09-ai-assessment/09-01-SUMMARY.md
@web_api/routes/assessments.py
@web_api/tests/conftest.py
@core/scoring.py
@core/assessments.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: RED -- Write failing tests for scoring trigger conditions</name>
  <files>web_api/tests/test_assessments_scoring.py</files>
  <action>
Create `web_api/tests/test_assessments_scoring.py` with tests for the scoring trigger BEFORE modifying the PATCH endpoint. These tests MUST fail initially -- that is the RED state.

The tests use unit+1 style: mock `enqueue_scoring` at the boundary (it is the async/external boundary -- behind it is the LLM call and DB write). The tests verify that the PATCH handler calls or does not call `enqueue_scoring` based on the request body. Also mock `core.assessments.update_response` to avoid needing a real database.

**Test class: TestScoringTrigger**

Use `@patch("web_api.routes.assessments.enqueue_scoring")` to mock the scoring function at the import location (it will be imported in assessments.py once wired). Also use `@patch("web_api.routes.assessments.update_response")` to mock the database call.

The mock `update_response` should return a dict representing a successful update:
```python
MOCK_ROW = {
    "response_id": 42,
    "question_id": "test-module:0:0",
    "module_slug": "test-module",
    "learning_outcome_id": "lo-1",
    "answer_text": "My answer",
    "answer_metadata": {},
    "created_at": "2026-01-01T00:00:00",
    "completed_at": "2026-01-01T00:00:00",
}
```

Use `httpx.AsyncClient` with the FastAPI app (or follow the pattern used in existing web_api tests -- check conftest.py and existing test files for the test client pattern used in this project).

Tests:

1. `test_patch_with_completed_at_triggers_scoring` -- PATCH `/api/assessments/responses/42` with body `{"completed_at": "2026-01-01T00:00:00Z"}`, mock update_response returns MOCK_ROW with completed_at set. Assert `enqueue_scoring` was called once with `response_id=42` and a `question_context` dict containing question_id, module_slug, learning_outcome_id, answer_text.

2. `test_patch_without_completed_at_does_not_trigger_scoring` -- PATCH `/api/assessments/responses/42` with body `{"answer_text": "updated draft"}`, mock update_response returns MOCK_ROW but with completed_at=None in body (body.completed_at is None). Assert `enqueue_scoring` was NOT called.

3. `test_patch_with_empty_completed_at_does_not_trigger_scoring` -- PATCH `/api/assessments/responses/42` with body `{"completed_at": ""}` (clearing completion). Assert `enqueue_scoring` was NOT called.

4. `test_post_does_not_trigger_scoring` -- POST `/api/assessments/responses` with a new response body. Mock `submit_response` to return a row. Assert `enqueue_scoring` was NOT called (POST creates drafts, never triggers scoring).

5. `test_patch_returns_without_waiting_for_scoring` -- PATCH with completed_at, assert the response returns 200 and `enqueue_scoring` was called (not awaited -- it is a sync function that creates a background task). This confirms the response is not blocked.

For auth mocking: use `@patch("web_api.routes.assessments.get_user_or_anonymous")` to return a mock auth tuple like `(UUID("00000000-0000-0000-0000-000000000001"), None)`. Or use the app's dependency override mechanism if the existing test patterns use that approach. Check existing web_api test files for the auth pattern.

For the database transaction mock: mock `web_api.routes.assessments.get_transaction` and `web_api.routes.assessments.get_connection` to return async context managers yielding mock connections. Follow the same patterns used in the existing web_api tests.

After writing the tests, run them. They MUST fail -- either ImportError (if enqueue_scoring is not yet imported in assessments.py) or AssertionError (if the function is never called). Confirm the RED state.
  </action>
  <verify>
Run `pytest web_api/tests/test_assessments_scoring.py -v` -- tests MUST FAIL (RED state).

Run `ruff check web_api/tests/test_assessments_scoring.py` -- lint must pass clean.

Confirm: 5 tests collected, all failing (either ImportError or AssertionError).
  </verify>
  <done>
5 failing tests exist in web_api/tests/test_assessments_scoring.py covering: PATCH with completed_at triggers scoring, PATCH without completed_at does not, PATCH with empty completed_at does not, POST does not trigger, response returns without waiting. All fail (RED state confirmed). Test file passes lint.
  </done>
</task>

<task type="auto">
  <name>Task 2: GREEN -- Wire scoring trigger into PATCH endpoint to pass all tests</name>
  <files>web_api/routes/assessments.py</files>
  <action>
Now modify `web_api/routes/assessments.py` to make all 5 trigger tests pass.

1. Add import at top of file:
```python
from core.scoring import enqueue_scoring
```

2. In the `update_assessment_response` function (the PATCH handler), after the `update_response` call succeeds and the 404 check passes, but before the return statement, add the scoring trigger:

```python
# Trigger AI scoring when response is completed (not on draft saves)
if body.completed_at and body.completed_at not in ("", "null"):
    enqueue_scoring(
        response_id=row["response_id"],
        question_context={
            "question_id": row["question_id"],
            "module_slug": row["module_slug"],
            "learning_outcome_id": row.get("learning_outcome_id"),
            "answer_text": row["answer_text"],
        },
    )
```

The trigger condition is:
- `body.completed_at` is truthy (not None -- meaning the request explicitly sets completed_at)
- `body.completed_at` is not empty string or "null" (those clear completed_at, not set it)

This means scoring fires when:
- PATCH with `completed_at: "2026-01-01T00:00:00Z"` (student finishes answer) -- YES, score
- PATCH with `answer_text: "updated draft"` only (auto-save during typing) -- NO, skip
- PATCH with `completed_at: ""` (clearing completion, if ever needed) -- NO, skip

Do NOT add scoring to the POST endpoint. POST creates a new response which is always a draft (no completed_at). Scoring only happens when the answer is finalized via PATCH.

Do NOT add any score data to any response model (SubmitResponseResponse, ResponseItem, ResponseListResponse). Per AI-04, scores are internal only.

After implementing, run all 5 tests. They MUST pass (GREEN state).

Then run the full test suite to confirm no regressions.
  </action>
  <verify>
Run `pytest web_api/tests/test_assessments_scoring.py -v` -- all 5 tests MUST PASS (GREEN state).

Run `ruff check web_api/routes/assessments.py` and `ruff format --check web_api/routes/assessments.py` -- both must pass clean.

Run `pytest` -- full test suite passes (no regressions).

Verify:
- `from core.scoring import enqueue_scoring` is in imports
- `enqueue_scoring(` appears inside `update_assessment_response` function only (not in POST)
- No score data appears in any response model

Confirm commit: `test(09-02): add failing tests for scoring trigger` (RED) and `feat(09-02): wire scoring trigger into PATCH endpoint` (GREEN).
  </verify>
  <done>
All 5 trigger tests pass. The PATCH endpoint triggers enqueue_scoring when completed_at is set to a valid timestamp. Draft saves (PATCH with only answer_text) do not trigger scoring. POST creates do not trigger scoring. No score data is exposed in any API response model. All existing tests pass. TDD RED-GREEN cycle complete with atomic commits. Phase 9 is complete.
  </done>
</task>

</tasks>

<verification>
1. `web_api/tests/test_assessments_scoring.py` has 5 passing tests in TestScoringTrigger class
2. `web_api/routes/assessments.py` contains `from core.scoring import enqueue_scoring`
3. Scoring trigger is in PATCH handler only, conditioned on `body.completed_at` being truthy and non-empty
4. No score data appears in any API response model
5. All existing tests pass unchanged
6. Full lint suite passes
7. Git history shows RED commit (failing tests) before GREEN commit (implementation)
8. Phase 9 requirements AI-01 through AI-04 are met
</verification>

<success_criteria>
TDD cycle complete: 5 failing tests written first (RED) covering all trigger conditions, then PATCH endpoint modified to pass them (GREEN). The assessment PATCH endpoint triggers background AI scoring when a response is marked complete. Draft saves do not trigger scoring. POST creates do not trigger scoring. The API response returns immediately without waiting for the LLM call. No score data is exposed to the frontend. All existing tests pass. Phase 9 is complete.
</success_criteria>

<output>
After completion, create `.planning/phases/09-ai-assessment/09-02-SUMMARY.md`
</output>
