---
phase: 09-ai-assessment
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - core/modules/llm.py
  - core/scoring.py
  - core/tests/test_scoring.py
  - core/__init__.py
autonomous: true

must_haves:
  truths:
    - "A non-streaming LLM completion function exists that supports structured JSON output via response_format"
    - "A scoring module can build distinct prompts for socratic vs assessment mode"
    - "The scoring module resolves question text, assessment prompt, and learning outcome name from the content cache using position-based question IDs"
    - "Background scoring tasks are tracked in a set to prevent garbage collection"
    - "Scoring failures are captured by Sentry and logged, never silently swallowed"
    - "Score data is written to the assessment_scores table with model_id and prompt_version"
  artifacts:
    - path: "core/modules/llm.py"
      provides: "Non-streaming complete() function with response_format support"
      contains: "async def complete"
    - path: "core/scoring.py"
      provides: "AI scoring module with prompt building, question resolution, background task management, and DB write"
      contains: "def enqueue_scoring"
    - path: "core/tests/test_scoring.py"
      provides: "Unit tests for prompt building, mode detection, and question resolution"
      contains: "class TestBuildScoringPrompt"
  key_links:
    - from: "core/scoring.py"
      to: "core/modules/llm.py"
      via: "complete() call in _score_response"
      pattern: "from core\\.modules\\.llm import complete"
    - from: "core/scoring.py"
      to: "core/modules/loader.py"
      via: "load_flattened_module in _resolve_question_details"
      pattern: "from core\\.modules\\.loader import load_flattened_module"
    - from: "core/scoring.py"
      to: "core/tables.py"
      via: "assessment_scores.insert() in _score_response"
      pattern: "from core\\.tables import assessment_scores"
---

<objective>
Create the AI scoring module and non-streaming LLM helper using TDD: RED tests first, then GREEN implementation.

Purpose: This is the scoring engine -- the module that builds prompts from question context, calls LiteLLM with structured output, and writes scores to the database. It must handle socratic vs assessment mode, resolve question details from the content cache, and run as a background task without blocking API responses.

Output: `core/tests/test_scoring.py` (new, written FIRST), `core/scoring.py` (new), `core/modules/llm.py` (modified), `core/__init__.py` (modified)
</objective>

<execution_context>
@/home/penguin/.claude/get-shit-done/workflows/execute-plan.md
@/home/penguin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-ai-assessment/09-RESEARCH.md
@core/modules/llm.py
@core/assessments.py
@core/tables.py
@core/modules/loader.py
@core/modules/flattened_types.py
@core/content/cache.py
@core/database.py
@core/tests/test_enrollment.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: RED -- Write failing tests for _build_scoring_prompt and _resolve_question_details</name>
  <files>core/tests/test_scoring.py</files>
  <action>
Create `core/tests/test_scoring.py` with unit tests BEFORE any production code exists. These tests MUST fail initially (ImportError or AssertionError) -- that is the RED state.

**Test class: TestBuildScoringPrompt**

Tests for `_build_scoring_prompt` -- a pure function that takes keyword args (answer_text, user_instruction, assessment_prompt, learning_outcome_name, mode) and returns a tuple of (system_prompt: str, messages: list[dict]).

Import it as: `from core.scoring import _build_scoring_prompt`

1. `test_socratic_mode_prompt_emphasizes_learning` -- mode="socratic" produces system prompt containing "supportive" or "effort" or "engagement" (not "rigorous")
2. `test_assessment_mode_prompt_emphasizes_measurement` -- mode="assessment" produces system prompt containing "rigorous" or "measure" (not "supportive")
3. `test_includes_learning_outcome_when_provided` -- learning_outcome_name="Understanding X" appears in system prompt
4. `test_excludes_learning_outcome_when_none` -- learning_outcome_name=None, system prompt does not contain "Learning Outcome:"
5. `test_includes_assessment_prompt_as_rubric` -- assessment_prompt="Check for X" appears in system prompt after "Scoring Rubric"
6. `test_excludes_assessment_prompt_when_none` -- assessment_prompt=None, no "Scoring Rubric" in system prompt
7. `test_user_message_contains_question_and_answer` -- returned messages list has one user message containing both user_instruction and answer_text
8. `test_returns_tuple_of_system_and_messages` -- return value is (str, list) with list length 1

**Test class: TestResolveQuestionDetails**

Tests for `_resolve_question_details` -- a sync function taking (module_slug: str, question_id: str) and returning a dict. These tests mock `core.scoring.load_flattened_module` (unit+1 style: mock the external boundary, which is the content cache/GitHub layer).

Import it as: `from core.scoring import _resolve_question_details`

Create a helper to build a mock FlattenedModule:
```python
def _make_module(sections):
    """Create a FlattenedModule with given sections list."""
    from core.modules.flattened_types import FlattenedModule
    return FlattenedModule(slug="test-module", title="Test", content_id=None, sections=sections)
```

Use `@patch("core.scoring.load_flattened_module")` on each test.

1. `test_resolves_question_in_test_section` -- section type="test" with question segment returns mode="assessment", correct user_instruction, assessment_prompt, learning_outcome_name
2. `test_resolves_question_in_page_section` -- section type="page" with question segment returns mode="socratic"
3. `test_returns_empty_for_invalid_question_id_format` -- question_id="invalid" returns {}
4. `test_returns_empty_for_out_of_bounds_section` -- section index beyond sections list returns {}
5. `test_returns_empty_for_out_of_bounds_segment` -- segment index beyond segments list returns {}
6. `test_returns_empty_for_non_question_segment` -- segment type="text" returns {}
7. `test_returns_empty_when_module_not_found` -- mock raises ModuleNotFoundError (from core.modules.loader), returns {}

Mock module sections structured as:
```python
[{
    "type": "test",  # or "page"
    "learningOutcomeName": "Test LO Name",
    "segments": [
        {"type": "question", "userInstruction": "Explain X", "assessmentPrompt": "Look for Y"}
    ]
}]
```

Follow existing test patterns in core/tests/ (class-based grouping, descriptive names, pytest style). These are all sync functions, no pytest.mark.asyncio needed.

After writing the tests, run them. They MUST fail (ImportError because core/scoring.py does not exist yet). Confirm the RED state.
  </action>
  <verify>
Run `pytest core/tests/test_scoring.py -v` -- tests MUST FAIL (ImportError: cannot import name '_build_scoring_prompt' from 'core.scoring').

Run `ruff check core/tests/test_scoring.py` -- lint must pass clean (tests themselves are valid Python).

Confirm: 15 tests collected, all failing with ImportError.
  </verify>
  <done>
15 failing tests exist in core/tests/test_scoring.py: 8 for _build_scoring_prompt and 7 for _resolve_question_details. All fail with ImportError (RED state confirmed). Test file passes lint.
  </done>
</task>

<task type="auto">
  <name>Task 2: GREEN -- Implement core/scoring.py and complete() in llm.py to pass all tests</name>
  <files>core/modules/llm.py, core/scoring.py, core/__init__.py</files>
  <action>
Now implement the production code to make all 15 tests pass.

**In core/modules/llm.py**, add a `complete()` function below the existing `stream_chat()`. This is a non-streaming counterpart for structured responses:

```python
async def complete(
    messages: list[dict],
    system: str,
    response_format: dict | None = None,
    provider: str | None = None,
    max_tokens: int = 1024,
) -> str:
```

- Uses `acompletion` (already imported) with no stream param (default is False)
- Prepends system message as `{"role": "system", "content": system}` (same pattern as stream_chat)
- Passes `response_format` kwarg only if provided
- Returns `response.choices[0].message.content`
- Uses `provider or DEFAULT_PROVIDER` for model selection (same pattern as stream_chat)

**Create core/scoring.py** with the following components:

1. **Module-level constants:**
   - `SCORING_PROVIDER = os.environ.get("SCORING_PROVIDER") or DEFAULT_PROVIDER` (import DEFAULT_PROVIDER from core.modules.llm)
   - `PROMPT_VERSION = "v1"` (for tracking in assessment_scores.prompt_version)
   - `_running_tasks: set[asyncio.Task] = set()` (prevents GC of background tasks)

2. **SCORE_SCHEMA** dict -- the `response_format` for structured output:
   ```python
   SCORE_SCHEMA = {
       "type": "json_schema",
       "json_schema": {
           "name": "assessment_score",
           "schema": {
               "type": "object",
               "properties": {
                   "overall_score": {"type": "integer", "description": "1-5 scale"},
                   "reasoning": {"type": "string", "description": "2-3 sentence explanation"},
                   "dimensions": {
                       "type": "object",
                       "additionalProperties": {
                           "type": "object",
                           "properties": {"score": {"type": "integer"}, "note": {"type": "string"}},
                           "required": ["score"]
                       }
                   },
                   "key_observations": {"type": "array", "items": {"type": "string"}}
               },
               "required": ["overall_score", "reasoning"],
               "additionalProperties": False
           }
       }
   }
   ```

3. **`enqueue_scoring(response_id: int, question_context: dict) -> None`** -- public API:
   - Creates an asyncio task via `asyncio.create_task(_score_response(...), name=f"score-{response_id}")`
   - Adds task to `_running_tasks` set
   - Adds `_task_done` callback via `task.add_done_callback()`

4. **`_task_done(task: asyncio.Task) -> None`** -- callback:
   - Removes task from `_running_tasks` via `.discard()`
   - If not cancelled and has exception: log error and `sentry_sdk.capture_exception(exc)`

5. **`_build_scoring_prompt(*, answer_text, user_instruction, assessment_prompt, learning_outcome_name, mode) -> tuple[str, list[dict]]`** -- pure function:
   - `mode == "socratic"`: supportive prompt emphasizing effort, engagement, learning progress
   - `mode == "assessment"`: rigorous prompt emphasizing precise rubric measurement
   - Appends learning_outcome_name to system prompt if available (prefixed with "Learning Outcome: ")
   - Appends assessment_prompt as "Scoring Rubric" to system prompt if available
   - Returns (system_prompt, [{"role": "user", "content": f"Question: {user_instruction}\n\nStudent's answer: {answer_text}\n\nScore this response according to the rubric."}])
   - See research Pattern 4 for exact prompt text

6. **`_resolve_question_details(module_slug: str, question_id: str) -> dict`** -- content lookup:
   - Imports `load_flattened_module, ModuleNotFoundError` from `core.modules.loader`
   - Parses `question_id` format "moduleSlug:sectionIndex:segmentIndex" by splitting on ":"
   - Validates 3 parts, converts indices to int, bounds-checks against module.sections and section segments
   - Checks segment type is "question"
   - Determines mode: `"assessment"` if `section.get("type") == "test"`, else `"socratic"`
   - Returns dict with keys: user_instruction, assessment_prompt, learning_outcome_name, mode
   - Returns empty dict `{}` on any lookup failure (module not found, invalid format, out of bounds, wrong segment type)
   - Logs warnings for failures

7. **`async _score_response(response_id: int, ctx: dict) -> None`** -- the scoring pipeline:
   - Calls `_resolve_question_details(ctx["module_slug"], ctx["question_id"])`
   - If empty dict returned, log warning and return (skip scoring)
   - Builds prompt via `_build_scoring_prompt()`
   - Calls `await complete()` with `SCORE_SCHEMA`, `SCORING_PROVIDER`, `max_tokens=512`
   - Parses JSON response via `json.loads()`
   - Writes to DB: `assessment_scores.insert().values(response_id=response_id, score_data=score_data, model_id=SCORING_PROVIDER, prompt_version=PROMPT_VERSION)`
   - Uses `async with get_transaction() as conn:` for DB write
   - Logs success with response_id and overall_score

Imports needed in scoring.py: `asyncio, json, logging, os`, `sentry_sdk`, `core.database.get_transaction`, `core.modules.llm.complete, DEFAULT_PROVIDER`, `core.tables.assessment_scores`, `core.modules.loader.load_flattened_module, ModuleNotFoundError`

**In core/__init__.py**, add the export:
```python
from .scoring import enqueue_scoring
```

After implementing, run all 15 tests. They MUST pass (GREEN state).
  </action>
  <verify>
Run `pytest core/tests/test_scoring.py -v` -- all 15 tests MUST PASS (GREEN state).

Run `ruff check core/modules/llm.py core/scoring.py` and `ruff format --check core/modules/llm.py core/scoring.py` -- both must pass clean.

Run `pytest` -- full test suite passes (no regressions).

Verify `enqueue_scoring` is importable: `python -c "from core import enqueue_scoring; print('OK')"` (may need event loop, but import itself should work).

Confirm commit: `test(09-01): add failing tests for scoring module` (RED) and `feat(09-01): implement scoring module` (GREEN).
  </verify>
  <done>
All 15 tests pass. core/scoring.py exists with enqueue_scoring (public), _score_response, _build_scoring_prompt, _resolve_question_details, _task_done, SCORE_SCHEMA, SCORING_PROVIDER, PROMPT_VERSION. core/modules/llm.py has complete() alongside existing stream_chat(). enqueue_scoring exported from core/__init__.py. Both files pass ruff lint and format checks. Full test suite passes with no regressions. TDD RED-GREEN cycle complete with atomic commits.
  </done>
</task>

</tasks>

<verification>
1. `core/tests/test_scoring.py` has 15 passing tests across 2 test classes
2. `core/scoring.py` exists with all 7 components (SCORE_SCHEMA, SCORING_PROVIDER, PROMPT_VERSION, enqueue_scoring, _task_done, _build_scoring_prompt, _resolve_question_details, _score_response)
3. `core/modules/llm.py` has both `stream_chat()` and `complete()` functions
4. `ruff check .` passes clean
5. `pytest` passes all tests (existing + new)
6. `from core import enqueue_scoring` works without error
7. Git history shows RED commit (failing tests) before GREEN commit (implementation)
</verification>

<success_criteria>
TDD cycle complete: 15 failing tests written first (RED), then production code implemented to pass them (GREEN). The AI scoring module exists as a self-contained unit in core/scoring.py with: structured score schema, socratic/assessment mode prompt building, content cache question resolution, asyncio background task management with Sentry error capture, and database write to assessment_scores. The complete() helper in llm.py provides non-streaming LLM calls with structured output. All functions are tested and the module is exported from core/__init__.py.
</success_criteria>

<output>
After completion, create `.planning/phases/09-ai-assessment/09-01-SUMMARY.md`
</output>
